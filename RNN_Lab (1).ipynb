{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XOkBF0K6P6MC"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow.keras.layers as tfkl\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "from google.colab import drive\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google.colab\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/9f/d3ec1275a089ec017f9c91af22ecd1e2fe738254b944e7a1f9528fcfacd0/google-colab-1.0.0.tar.gz (72kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 2.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth~=1.4.0 (from google.colab)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/80/369a47c28ce7d9be6a6973338133d073864d8efbb62747e414c34a3a5f4f/google_auth-1.4.2-py2.py3-none-any.whl (64kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 7.1MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting ipykernel~=4.6.0 (from google.colab)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/c3/76775a650cae2e3d9c033b26153583e61282692d9a3af12a3022d8f0cefa/ipykernel-4.6.1-py3-none-any.whl (104kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 12.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ipython~=5.5.0 (from google.colab)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/63/c987612bcf82c56eaacaf6bf01e31e53a244a0a3a0fb036ec5adc377e0fe/ipython-5.5.0-py3-none-any.whl (758kB)\n",
      "\u001b[K     |████████████████████████████████| 768kB 14.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting notebook~=5.2.0 (from google.colab)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/a2/d951ebb7855743f989287486ded73cd3f66915c3ecd9c5c5a0c7ca12377a/notebook-5.2.2-py2.py3-none-any.whl (8.0MB)\n",
      "\u001b[K     |████████████████████████████████| 8.0MB 1.2MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting six~=1.12.0 (from google.colab)\n",
      "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Collecting pandas~=0.24.0 (from google.colab)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/43/fd867e3347559845c8f993059d410c50a1e18709f1c4d4b3b47323a06a37/pandas-0.24.2-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (15.9MB)\n",
      "\u001b[K     |████████████████████████████████| 15.9MB 2.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting portpicker~=1.2.0 (from google.colab)\n",
      "  Downloading https://files.pythonhosted.org/packages/49/2c/a75ef568273036aa61319a554164e6031e31708106ea6ca10e17265e1703/portpicker-1.2.0.tar.gz\n",
      "Collecting requests~=2.21.0 (from google.colab)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 2.8MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting tornado~=4.5.0 (from google.colab)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/7b/e29ab3d51c8df66922fea216e2bddfcb6430fb29620e5165b16a216e0d3c/tornado-4.5.3.tar.gz (484kB)\n",
      "\u001b[K     |████████████████████████████████| 491kB 2.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cachetools>=2.0.0 in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from google-auth~=1.4.0->google.colab) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from google-auth~=1.4.0->google.colab) (0.2.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from google-auth~=1.4.0->google.colab) (4.6)\n",
      "Requirement already satisfied: traitlets>=4.1.0 in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from ipykernel~=4.6.0->google.colab) (4.3.2)\n",
      "Requirement already satisfied: jupyter-client in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from ipykernel~=4.6.0->google.colab) (5.3.1)\n",
      "Collecting prompt-toolkit<2.0.0,>=1.0.4 (from ipython~=5.5.0->google.colab)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/27/5fd61a451d086ad4aa806dc72fe1383d2bc0e74323668672287f616d5d51/prompt_toolkit-1.0.18-py3-none-any.whl (245kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 2.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from ipython~=5.5.0->google.colab) (46.1.3)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from ipython~=5.5.0->google.colab) (0.8.1)\n",
      "Requirement already satisfied: pygments in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from ipython~=5.5.0->google.colab) (2.4.2)\n",
      "Requirement already satisfied: pickleshare in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from ipython~=5.5.0->google.colab) (0.7.5)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from ipython~=5.5.0->google.colab) (0.1.0)\n",
      "Requirement already satisfied: decorator in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from ipython~=5.5.0->google.colab) (4.4.2)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from ipython~=5.5.0->google.colab) (4.7.0)\n",
      "Requirement already satisfied: jinja2 in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from notebook~=5.2.0->google.colab) (2.11.2)\n",
      "Requirement already satisfied: ipython-genutils in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from notebook~=5.2.0->google.colab) (0.2.0)\n",
      "Requirement already satisfied: nbconvert in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from notebook~=5.2.0->google.colab) (5.5.0)\n",
      "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from notebook~=5.2.0->google.colab) (0.8.2)\n",
      "Requirement already satisfied: nbformat in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from notebook~=5.2.0->google.colab) (4.4.0)\n",
      "Requirement already satisfied: jupyter-core in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from notebook~=5.2.0->google.colab) (4.5.0)\n",
      "Requirement already satisfied: pytz>=2011k in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from pandas~=0.24.0->google.colab) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from pandas~=0.24.0->google.colab) (1.18.4)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from pandas~=0.24.0->google.colab) (2.8.1)\n",
      "Collecting urllib3<1.25,>=1.21.1 (from requests~=2.21.0->google.colab)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/11/525b02e4acc0c747de8b6ccdab376331597c569c42ea66ab0a1dbd36eca2/urllib3-1.24.3-py2.py3-none-any.whl (118kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 1.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from requests~=2.21.0->google.colab) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from requests~=2.21.0->google.colab) (2020.4.5.1)\n",
      "Collecting idna<2.9,>=2.5 (from requests~=2.21.0->google.colab)\n",
      "  Using cached https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth~=1.4.0->google.colab) (0.4.8)\n",
      "Requirement already satisfied: pyzmq>=13 in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from jupyter-client->ipykernel~=4.6.0->google.colab) (18.0.0)\n",
      "Requirement already satisfied: wcwidth in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython~=5.5.0->google.colab) (0.1.7)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython~=5.5.0->google.colab) (0.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from jinja2->notebook~=5.2.0->google.colab) (1.1.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from nbconvert->notebook~=5.2.0->google.colab) (1.4.2)\n",
      "Requirement already satisfied: bleach in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from nbconvert->notebook~=5.2.0->google.colab) (3.1.0)\n",
      "Requirement already satisfied: testpath in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.4.2)\n",
      "Requirement already satisfied: mistune>=0.8.1 in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.8.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.3)\n",
      "Requirement already satisfied: defusedxml in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.6.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from nbformat->notebook~=5.2.0->google.colab) (3.0.1)\n",
      "Requirement already satisfied: webencodings in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from bleach->nbconvert->notebook~=5.2.0->google.colab) (0.5.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook~=5.2.0->google.colab) (19.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook~=5.2.0->google.colab) (0.14.11)\n",
      "Building wheels for collected packages: google.colab, portpicker, tornado\n",
      "  Building wheel for google.colab (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/liuhao/Library/Caches/pip/wheels/38/0d/59/701e300a337b2a2e07b27fe74dbfff0bc56ac58f711566ee67\n",
      "  Building wheel for portpicker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/liuhao/Library/Caches/pip/wheels/4a/45/47/1e126be9d4605e71f00d6e6fb151611f2f4cb9770b050c7d2d\n",
      "  Building wheel for tornado (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/liuhao/Library/Caches/pip/wheels/72/bf/f4/b68fa69596986881b397b18ff2b9af5f8181233aadcc9f76fd\n",
      "Successfully built google.colab portpicker tornado\n",
      "\u001b[31mERROR: astroid 2.2.5 requires typed-ast>=1.3.0; implementation_name == \"cpython\", which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: xarray 0.15.1 has requirement pandas>=0.25, but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorboard 2.3.0 has requirement google-auth<2,>=1.6.3, but you'll have google-auth 1.4.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: spyder-kernels 0.5.1 has requirement ipykernel>=4.8.2, but you'll have ipykernel 4.6.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: jupyter-console 6.0.0 has requirement prompt_toolkit<2.1.0,>=2.0.0, but you'll have prompt-toolkit 1.0.18 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: distributed 2.15.2 has requirement tornado>=5; python_version < \"3.8\", but you'll have tornado 4.5.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: cufflinks 0.16 has requirement plotly<4.0.0a0,>=3.0.0, but you'll have plotly 4.1.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: bokeh 2.0.2 has requirement tornado>=5, but you'll have tornado 4.5.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: six, google-auth, prompt-toolkit, ipython, tornado, ipykernel, notebook, pandas, portpicker, urllib3, idna, requests, google.colab\n",
      "  Found existing installation: six 1.14.0\n",
      "    Uninstalling six-1.14.0:\n",
      "      Successfully uninstalled six-1.14.0\n",
      "  Found existing installation: google-auth 1.22.1\n",
      "    Uninstalling google-auth-1.22.1:\n",
      "      Successfully uninstalled google-auth-1.22.1\n",
      "  Found existing installation: prompt-toolkit 2.0.9\n",
      "    Uninstalling prompt-toolkit-2.0.9:\n",
      "      Successfully uninstalled prompt-toolkit-2.0.9\n",
      "  Found existing installation: ipython 7.6.1\n",
      "    Uninstalling ipython-7.6.1:\n",
      "      Successfully uninstalled ipython-7.6.1\n",
      "  Found existing installation: tornado 6.0.4\n",
      "    Uninstalling tornado-6.0.4:\n",
      "      Successfully uninstalled tornado-6.0.4\n",
      "  Found existing installation: ipykernel 5.1.1\n",
      "    Uninstalling ipykernel-5.1.1:\n",
      "      Successfully uninstalled ipykernel-5.1.1\n",
      "  Found existing installation: notebook 6.0.0\n",
      "    Uninstalling notebook-6.0.0:\n",
      "      Successfully uninstalled notebook-6.0.0\n",
      "  Found existing installation: pandas 1.0.3\n",
      "    Uninstalling pandas-1.0.3:\n",
      "      Successfully uninstalled pandas-1.0.3\n",
      "  Found existing installation: urllib3 1.25.9\n",
      "    Uninstalling urllib3-1.25.9:\n",
      "      Successfully uninstalled urllib3-1.25.9\n",
      "  Found existing installation: idna 2.9\n",
      "    Uninstalling idna-2.9:\n",
      "      Successfully uninstalled idna-2.9\n",
      "  Found existing installation: requests 2.23.0\n",
      "    Uninstalling requests-2.23.0:\n",
      "      Successfully uninstalled requests-2.23.0\n",
      "Successfully installed google-auth-1.4.2 google.colab idna-2.8 ipykernel-4.6.1 ipython-5.5.0 notebook-5.2.2 pandas-1.0.3 portpicker-1.2.0 prompt-toolkit-1.0.18 requests-2.22.0 six-1.12.0 tornado-6.0.3 urllib3-1.24.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google.colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdCU982WwzFo"
   },
   "source": [
    "In this example, we're going to train a [CharRNN](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) on a body of Shakespearian text. Ultimtely, this is an unsuperived learning task. But similar to our previous explorations in unsupervised DL, we will use an unlabeled dataset and create many samples of labeled data that we can use with our familiar supervised loss functions. The result will be a model that has learned the statistical properties of the input text, and can then be considered a \"generative\" model of language because we can use it to generate synthetic passages of Shakespeare.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dX7qrncTRKN0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6iek9QSARq1L"
   },
   "outputs": [],
   "source": [
    "file_path = \"./shakespeare.txt\"\n",
    "\n",
    "with open(file_path,\"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ie2LtLF4Vv6A"
   },
   "source": [
    "We've loaded our Shakespeare text, let's take a look at a random snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LVFmTUsGWePe",
    "outputId": "b03893e8-f3c8-4d8e-e8b0-9e61699f440f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lies i' the second chamber?\n",
      "  LADY MACBETH. Donalbain.\n",
      "  MACBETH. This is a sorry sight.           [Looks on his hands.\n",
      "  LADY MACBETH. A foolish thought, to say a sorry sight.\n",
      "  MACBETH. There's one did laugh in 's sleep, and one cried,\n",
      "      \"Murther!\"\n",
      "    That they did wake each other. I stood and heard them,\n",
      "    But they did say their prayers and address'd them\n",
      "    Again to sleep.\n",
      "  LADY MACB\n"
     ]
    }
   ],
   "source": [
    "print(text[31600:32000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLXQHFUsW0xu"
   },
   "source": [
    "We need to convert our text into numeric arrays, the next several blocks accomplish this.\n",
    "\n",
    "First, we'll create a mapping between characters and their numeric index. We'll also create the reverse mapping, which is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UkvcQEUASXQG",
    "outputId": "bd84d04c-0e0a-48a1-e5dc-999c683a80c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 75\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XexyPZdAXC0p"
   },
   "source": [
    "Next, we'll create a training set of sub-sequences. Remember, we're trying to train a model to be able to predict the next chracter if it is given several characters of a subsequence. So we will create training pairs where each X is a fixed-length subsequences and each Y is the corresponding next letter in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ej4RdC76S7RB",
    "outputId": "83f06f97-9ec9-4b84-ac86-5ea7fe4deee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 38700\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "step = 3\n",
    "sub_sequences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sub_sequences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sub_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QVHru3qPWX8Z",
    "outputId": "00758db3-ed36-4be3-df25-0fa7c46d5fcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Sequence):\n",
      " and other Apparitions\n",
      "  Lords, Gentleme\n",
      "\n",
      "(Target Character): \n",
      "n\n"
     ]
    }
   ],
   "source": [
    "k=300\n",
    "print(\"(Sequence):\\n\" + sub_sequences[k])\n",
    "print(\"\\n(Target Character): \\n\" + next_chars[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vD2QxlOAW8zQ"
   },
   "source": [
    "Next we'll create one-hot vectors for our sub-sequences. The tensor we create here will be shaped as (num_sequences x sequence_length x alphabet_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SfQRBmiNWehk"
   },
   "outputs": [],
   "source": [
    "X = np.zeros((len(sub_sequences), maxlen, len(chars)), dtype=np.uint8 )\n",
    "Y = np.zeros((len(sub_sequences), len(chars)), dtype=np.uint8)\n",
    "for i, seq in enumerate(sub_sequences):\n",
    "    for t, char in enumerate(seq):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "        Y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4qxjsGDXLtb",
    "outputId": "79472792-5f85-4e21-b6bb-41481cb532a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "423pgyKqXnE_",
    "outputId": "38e5fd04-3637-4463-ab2d-55ffb340830a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dJrr1caYVnI"
   },
   "source": [
    "Our RNN model will be quite simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "95NSRVMpYGAT"
   },
   "outputs": [],
   "source": [
    "char_rnn = Sequential()\n",
    "char_rnn.add(tfkl.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "char_rnn.add(tfkl.Dense(len(chars),activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "t4xdUMP_Y6iu"
   },
   "outputs": [],
   "source": [
    "char_rnn.compile(loss='categorical_crossentropy', optimizer=tfk.optimizers.RMSprop(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "KGDTEd0GZFNk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "38/38 [==============================] - 10s 263ms/step - loss: 3.2365\n",
      "Epoch 2/20\n",
      "38/38 [==============================] - 10s 253ms/step - loss: 2.7947\n",
      "Epoch 3/20\n",
      "38/38 [==============================] - 10s 250ms/step - loss: 2.3645\n",
      "Epoch 4/20\n",
      "38/38 [==============================] - 10s 252ms/step - loss: 2.0886\n",
      "Epoch 5/20\n",
      "38/38 [==============================] - 10s 252ms/step - loss: 1.9325\n",
      "Epoch 6/20\n",
      "38/38 [==============================] - 10s 251ms/step - loss: 1.8189\n",
      "Epoch 7/20\n",
      "38/38 [==============================] - 10s 253ms/step - loss: 1.7277\n",
      "Epoch 8/20\n",
      "38/38 [==============================] - 10s 253ms/step - loss: 1.6476\n",
      "Epoch 9/20\n",
      "38/38 [==============================] - 10s 252ms/step - loss: 1.5803\n",
      "Epoch 10/20\n",
      "38/38 [==============================] - 10s 251ms/step - loss: 1.5265\n",
      "Epoch 11/20\n",
      "38/38 [==============================] - 10s 251ms/step - loss: 1.4714\n",
      "Epoch 12/20\n",
      "38/38 [==============================] - 10s 252ms/step - loss: 1.4264\n",
      "Epoch 13/20\n",
      "38/38 [==============================] - 10s 253ms/step - loss: 1.3825\n",
      "Epoch 14/20\n",
      "38/38 [==============================] - 10s 252ms/step - loss: 1.3357\n",
      "Epoch 15/20\n",
      "38/38 [==============================] - 10s 276ms/step - loss: 1.2947\n",
      "Epoch 16/20\n",
      "38/38 [==============================] - 10s 253ms/step - loss: 1.2551\n",
      "Epoch 17/20\n",
      "38/38 [==============================] - 10s 250ms/step - loss: 1.2094\n",
      "Epoch 18/20\n",
      "38/38 [==============================] - 9s 250ms/step - loss: 1.1703\n",
      "Epoch 19/20\n",
      "38/38 [==============================] - 10s 251ms/step - loss: 1.1317\n",
      "Epoch 20/20\n",
      "38/38 [==============================] - 10s 255ms/step - loss: 1.0957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8165d43828>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_rnn.fit(X,Y, epochs=20, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hhAWPgRX96V"
   },
   "source": [
    "Once we have a trained model, we can simulate new text by making predictions about the next character and then drawing characters in proportion to the predicted probabilities. And then simple repeat that process over and over, each time drawing the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "IMpJwYSsZSoc"
   },
   "outputs": [],
   "source": [
    "def draw_char(probs):\n",
    "    probs = np.asarray(probs).astype('float64')\n",
    "    if sum(probs) != 1.0:\n",
    "        probs = probs / np.sum(probs)\n",
    "    draw = np.random.choice(range(len(probs)) , p=probs)\n",
    "    return draw\n",
    "\n",
    "def sample_text(model, sample_length=100):\n",
    "    start = np.random.randint(0, len(text) - maxlen - 1)\n",
    "    sequence = text[start: start + maxlen]\n",
    "  \n",
    "    x_preds = np.zeros((sample_length, maxlen, len(chars)))\n",
    "    for i in range(sample_length):\n",
    "        for t, char in enumerate(sequence[-maxlen:]):\n",
    "            x_preds[i, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(np.expand_dims(x_preds[i,:,:], axis=0), verbose=0)[0]\n",
    "        next_index = draw_char(preds)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        sequence += next_char\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jHD5iDlHayL7"
   },
   "outputs": [],
   "source": [
    "sim = sample_text(char_rnn,sample_length=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bOP0ljRtOEmp",
    "outputId": "6cf5352e-bf0f-49f0-d254-f49732856443"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at's shot\n",
      "    Hath not yet lighted, and we be well to pe\n",
      "    I sen seme to all we his haven but of but to now.\n",
      "    Of keill from the rast hade of wonde.\n",
      "    The loring of forreiss to you an y! Mecteld,\n",
      "s ull coundaus, horrous with munt his haver!                     Entere. Slook to have lard to I, lo, mes fuld to their bent words,\n",
      "    But not swanged at to come no youlp.\n",
      "    Still hawe a hame Gon, honal that my and\n",
      "    The spot to is's flets agat Is't smeet in her fion\n",
      "    I bark not vealy but this I have lid do more.\n",
      "    With 'tim w\n"
     ]
    }
   ],
   "source": [
    "print(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aj4kXg4BTbOc"
   },
   "source": [
    "Notice that we can do pretty well to learn the typical statistical patterns of this text and then simulate new text that appears to be very similar to legitimate Shakespeare. \n",
    "\n",
    "But just a caution - we can also do pretty well with a much simpler method (Markov model): http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139\n",
    "\n",
    "So the lesson is to try something simple before jumping right in to deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5IE5xprp3RS"
   },
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoN_s6nQsDdn"
   },
   "source": [
    "In this example, we're going to use an RNN for sequence classification. The task we'll set up is to generate a training set of randomized strings, and train our model to detect whether a string contains any vowels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Isy5RPDdsTYT"
   },
   "source": [
    "First, we'll create a training dataset of short randomized character sequences and the corresponding label of whether or not they contain at least one vowel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "hE6C-Xl6p5W7"
   },
   "outputs": [],
   "source": [
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "CI7QA2Ewp-ZJ"
   },
   "outputs": [],
   "source": [
    "def contains_vowels(sequence):\n",
    "    vowels = [\"a\", \"e\", \"i\", \"o\", \"u\"]\n",
    "    return any([vowel in list(sequence) for vowel in vowels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oZ9cEhMrqtoG",
    "outputId": "17d2110a-4251-4ace-fe84-50bb30362d2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contains_vowels(\"gradient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "EwBEUPYwp9Z0"
   },
   "outputs": [],
   "source": [
    "sequences = []\n",
    "labels = []\n",
    "for i in range(1000):\n",
    "    char_list = np.random.choice( list(string.ascii_lowercase), size = 5, replace=True)\n",
    "    seq = \"\".join(char_list)\n",
    "    sequences.append(seq)\n",
    "    labels.append(int(contains_vowels(seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "IkJXdy5krgHn"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"sequence\": sequences, \"label\":labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas.compat\n",
      "  Downloading https://files.pythonhosted.org/packages/ce/8d/1b8899ea7522590ec05d57f7fe3cb86044603032c5b03f3a08d0c3566828/pandas_compat-0.1.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pandas in /Users/liuhao/anaconda3/lib/python3.7/site-packages (from pandas.compat) (1.0.3)\n",
      "\u001b[33mWARNING: No metadata found in /Users/liuhao/anaconda3/lib/python3.7/site-packages\u001b[0m\n",
      "\u001b[31mERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: '/Users/liuhao/anaconda3/lib/python3.7/site-packages/pandas-1.0.3.dist-info/METADATA'\n",
      "\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas.compat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ubAQf53Dr8zy",
    "outputId": "9ac91bda-3c6a-4e22-833a-44eb918e16d6"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OrderedDict' from 'pandas.compat' (/Users/liuhao/anaconda3/lib/python3.7/site-packages/pandas/compat/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;34m\"\"\"map in to lookup_by_type\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup_by_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0mProperty\u001b[0m \u001b[0mreturning\u001b[0m \u001b[0ma\u001b[0m \u001b[0mStyler\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mmethods\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0mbuilding\u001b[0m \u001b[0ma\u001b[0m \u001b[0mstyled\u001b[0m \u001b[0mHTML\u001b[0m \u001b[0mrepresentation\u001b[0m \u001b[0mfo\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0mAlso\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_html\u001b[0;34m(self, buf, encoding, classes, notebook, border)\u001b[0m\n\u001b[1;32m    977\u001b[0m                     \u001b[0;31m# False specifically, so that the default is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m                     \u001b[0;31m# to include a space if we get here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m                     \u001b[0mtpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mu'{v}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                     \u001b[0mtpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mu' {v}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/formats/html.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextwrap\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdedent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlzip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munichr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mABCMultiIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'OrderedDict' from 'pandas.compat' (/Users/liuhao/anaconda3/lib/python3.7/site-packages/pandas/compat/__init__.py)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  sequence  label\n",
       "0    qxmxq      0\n",
       "1    wmiiu      1\n",
       "2    wnqwi      1\n",
       "3    rvegz      1\n",
       "4    ujswg      1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xkX8Xa8sfID"
   },
   "source": [
    "Next, set up and train an RNN (of any type) to solve this task. What preprocessing will you need to do first on the raw data in order to prepare it for the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "6M4Sj4XHr9hj"
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "# your code here\n",
    "char0 = list(string.ascii_lowercase)\n",
    "char_indices0 = dict((c, i) for i, c in enumerate(char0))\n",
    "indices_char0 = dict((i, c) for i, c in enumerate(char0))\n",
    "X = np.zeros((len(sequences),5,26), dtype=np.uint8)\n",
    "for i, seq in enumerate(sequences):\n",
    "    for t,char in enumerate(seq):\n",
    "        X[i,t,char_indices0[char]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Og8gtSgHslED"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               79360     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 79,489\n",
      "Trainable params: 79,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model setup and training\n",
    "# your code here\n",
    "myrnn = Sequential()\n",
    "myrnn.add(tfkl.LSTM(128, input_shape=(5,26)))\n",
    "myrnn.add(tfkl.Dense(1,activation=\"sigmoid\"))\n",
    "myrnn.compile(loss='binary_crossentropy', optimizer=tfk.optimizers.RMSprop(lr=0.01),metrics=['accuracy'])\n",
    "myrnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "lSLXwJvJszDu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "20/20 [==============================] - 0s 15ms/step - loss: 0.5502 - accuracy: 0.8168\n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 0s 15ms/step - loss: 0.1656 - accuracy: 0.9527\n",
      "Epoch 3/20\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
      " 0/20 [..............................] - 0s 0s/step - loss: 0.1656 - accuracy: 0.9527\n"
     ]
    }
   ],
   "source": [
    "Y = np.array(labels)\n",
    "results = myrnn.fit(X,Y,epochs=20,steps_per_epoch=20,batch_size=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## idk why I do not have enough data, but will try to figure out later"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNN Lab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
